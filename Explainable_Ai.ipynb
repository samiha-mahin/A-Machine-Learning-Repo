{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVrrRVIKdTpWtGCYDLdb61",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/A-Machine-Learning-Models-Repo/blob/main/Explainable_Ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Model-Based Explainable AI (XAI)**\n",
        "\n",
        "## üîç What is Model-Based Explainable AI?\n",
        "\n",
        "**Model-Based Explainable AI** refers to techniques where **the model itself is inherently interpretable or designed to be explainable**. These models are either:\n",
        "\n",
        "1. **Intrinsically interpretable**: like Decision Trees, Linear Regression.\n",
        "2. **Modified to provide explanations**: for example, using attention mechanisms in neural networks or building inherently explainable deep models.\n",
        "\n",
        "So, in **Model-Based XAI**, the **focus is on building a model that can explain its decisions as part of its design**, instead of using post-hoc tools like SHAP or LIME.\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Key Characteristics\n",
        "\n",
        "* Interpretability is built into the model.\n",
        "* No need for external explanation tools.\n",
        "* Often involves a tradeoff between accuracy and interpretability.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Examples of Model-Based XAI\n",
        "\n",
        "### 1. **Decision Trees (e.g., CART, ID3)**\n",
        "\n",
        "* The tree structure shows how decisions are made based on features.\n",
        "* At each node, a rule is applied (e.g., \"Age < 30?\"), making it easy to follow and understand.\n",
        "\n",
        "üìå **Use case**: Predicting if a loan should be approved.\n",
        "\n",
        "```plaintext\n",
        "If income > 50K:\n",
        "   ‚îî‚îÄ‚îÄ If credit score > 700 ‚Üí Approve\n",
        "   ‚îî‚îÄ‚îÄ Else ‚Üí Deny\n",
        "Else:\n",
        "   ‚îî‚îÄ‚îÄ Deny\n",
        "```\n",
        "\n",
        "Here, you can clearly see **why** the loan was approved or denied.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Generalized Additive Models (GAMs)**\n",
        "\n",
        "* Predicts outcome as a sum of functions of individual features.\n",
        "* You can visualize each feature's contribution.\n",
        "\n",
        "üìå **Use case**: Predicting diabetes risk.\n",
        "\n",
        "The model might say:\n",
        "\n",
        "```plaintext\n",
        "Risk score = f1(age) + f2(BMI) + f3(blood pressure)\n",
        "```\n",
        "\n",
        "Each `f` is a smooth curve showing how that feature affects the prediction. Easy to interpret.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Explainable Boosting Machines (EBMs)**\n",
        "\n",
        "* An extension of GAMs, based on boosted decision trees.\n",
        "* High accuracy + interpretability.\n",
        "\n",
        "üìå **Use case**: Hospital readmission prediction.\n",
        "\n",
        "You can see:\n",
        "\n",
        "* Feature: \"No. of past visits\" ‚Üí contributes +10 to risk.\n",
        "* Feature: \"Age > 70\" ‚Üí contributes +15 to risk.\n",
        "\n",
        "Each feature‚Äôs contribution is shown clearly.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Attention-based models (in NLP or Vision)**\n",
        "\n",
        "* Highlight **which part of the input** the model focused on.\n",
        "\n",
        "üìå **Use case**: In sentiment analysis\n",
        "\n",
        "* Sentence: \"The movie was surprisingly good\"\n",
        "* Attention weights show \"surprisingly good\" had the most impact on prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **ProtoPNet (Prototype Networks)**\n",
        "\n",
        "* Used in image classification.\n",
        "* Model classifies based on **prototype images** from training set.\n",
        "\n",
        "üìå **Use case**: Classifying birds\n",
        "\n",
        "* The model says: \"I classify this as a Sparrow because this part of the bird looks like this part in a known Sparrow image.\"\n",
        "\n",
        "So it's like saying **\"This new sample is similar to this known prototype\"**, which helps in human-level understanding.\n",
        "\n",
        "---\n",
        "\n",
        "## üü¢ Summary\n",
        "\n",
        "| Model-Based XAI | Description                            | Example               |\n",
        "| --------------- | -------------------------------------- | --------------------- |\n",
        "| Decision Trees  | Rule-based structure                   | Loan approval         |\n",
        "| GAMs            | Sum of interpretable feature functions | Diabetes prediction   |\n",
        "| EBMs            | Boosted interpretable model            | Hospital readmission  |\n",
        "| Attention       | Highlights input regions               | Sentiment/translation |\n",
        "| ProtoPNet       | Uses prototypes for decision           | Image classification  |\n",
        "\n",
        "---\n",
        "\n",
        "## üìå When to Use Model-Based XAI?\n",
        "\n",
        "* When you **need trust and clarity** (e.g., healthcare, law, finance).\n",
        "* When regulatory compliance requires **explainability**.\n",
        "* When end-users need to understand the **why** behind a prediction.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CADC8O-jgAXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Post-Hoc Explainable AI (XAI)**\n",
        "\n",
        "## üîç What is Post-Hoc Explainable AI?\n",
        "\n",
        "**Post-Hoc XAI** means explaining a model **after it has already been trained**, **without changing the model itself**.\n",
        "\n",
        "Think of it like this:\n",
        "\n",
        "> ‚ÄúThe model is already made (like a black box). Now let‚Äôs try to understand **why it gave a certain output**, using tools and methods.‚Äù\n",
        "\n",
        "This approach is useful when the model is too complex (e.g., deep neural networks) or when we want to keep accuracy while adding interpretability.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Categories of Post-Hoc Explanations\n",
        "\n",
        "| Type                    | Explanation                                                |\n",
        "| ----------------------- | ---------------------------------------------------------- |\n",
        "| **Global**              | Explains the overall behavior of the model.                |\n",
        "| **Local**               | Explains why the model made a specific prediction.         |\n",
        "| **Feature-based**       | Shows feature importance.                                  |\n",
        "| **Example-based**       | Uses similar past examples to explain.                     |\n",
        "| **Visualization-based** | Uses heatmaps or attention maps to show influential parts. |\n",
        "\n",
        "---\n",
        "\n",
        "## üü• Post-Hoc XAI for Black-Box Models\n",
        "\n",
        "### ‚úÖ Common Black-Box Models\n",
        "\n",
        "* Deep Neural Networks (CNNs, LSTMs)\n",
        "* Random Forests\n",
        "* Gradient Boosted Trees\n",
        "* SVMs with nonlinear kernels\n",
        "\n",
        "### üìå Tools & Examples\n",
        "\n",
        "#### 1. **SHAP (SHapley Additive exPlanations)**\n",
        "\n",
        "* Based on game theory.\n",
        "* Tells you how much each feature contributed to a specific prediction.\n",
        "\n",
        "üìç **Example**: In credit scoring\n",
        "\n",
        "* Model predicts 85% chance of loan default.\n",
        "* SHAP explains:\n",
        "\n",
        "  * Age: -12%\n",
        "  * Low income: +20%\n",
        "  * High debt: +15%\n",
        "  * Total = 85%\n",
        "\n",
        "> \"Because of your low income and high debt, the model predicted a high risk.\"\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **LIME (Local Interpretable Model-agnostic Explanations)**\n",
        "\n",
        "* Creates a simple interpretable model (like a linear model) **around a single prediction**.\n",
        "* Helps understand one prediction at a time.\n",
        "\n",
        "üìç **Example**: Text sentiment\n",
        "\n",
        "> Sentence: ‚ÄúThe movie was **not good**.‚Äù\n",
        "\n",
        "* LIME shows:\n",
        "\n",
        "  * ‚Äúnot‚Äù = -0.6\n",
        "  * ‚Äúgood‚Äù = +0.4\n",
        "    So, ‚Äúnot‚Äù had stronger negative influence.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Saliency Maps / Grad-CAM (for images)**\n",
        "\n",
        "* Used for CNNs.\n",
        "* Highlights which part of an image the model used to make its prediction.\n",
        "\n",
        "üìç **Example**: Dog vs. cat classifier\n",
        "\n",
        "* Grad-CAM highlights the ears and nose of the cat in red ‚Äî the parts that helped it decide it‚Äôs a cat.\n",
        "\n",
        "---\n",
        "\n",
        "## üü® Post-Hoc XAI for White-Box Models\n",
        "\n",
        "Even though white-box models are already interpretable, we sometimes use post-hoc tools for:\n",
        "\n",
        "* Better **visualization**\n",
        "* Feature ranking\n",
        "* Comparing **real behavior vs. expected logic**\n",
        "\n",
        "### ‚úÖ Common White-Box Models\n",
        "\n",
        "* Decision Trees\n",
        "* Linear Regression\n",
        "* Logistic Regression\n",
        "* Rule-based models\n",
        "\n",
        "### üìå Tools & Examples\n",
        "\n",
        "#### 1. **Feature Importance Plots**\n",
        "\n",
        "* Ranks features by how much they influence the output.\n",
        "* Can be based on:\n",
        "\n",
        "  * Coefficients (in regression)\n",
        "  * Gini importance (in trees)\n",
        "\n",
        "üìç **Example**: House price prediction\n",
        "\n",
        "* Top features:\n",
        "\n",
        "  * Area: 45%\n",
        "  * Location: 30%\n",
        "  * Age of house: 25%\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Partial Dependence Plots (PDPs)**\n",
        "\n",
        "* Shows how one feature affects the prediction, keeping others constant.\n",
        "\n",
        "üìç **Example**: Predicting diabetes risk\n",
        "\n",
        "* Plot for \"BMI\" shows that risk increases rapidly after BMI > 28.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **ICE (Individual Conditional Expectation) plots**\n",
        "\n",
        "* Like PDP, but for **individual predictions**.\n",
        "* Helps in fairness analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## üßæ Summary Table\n",
        "\n",
        "| Model Type    | Post-Hoc XAI Tools                                           | Purpose                                            |\n",
        "| ------------- | ------------------------------------------------------------ | -------------------------------------------------- |\n",
        "| **Black-Box** | SHAP, LIME, Grad-CAM, Saliency Maps                          | Explain opaque models like DNNs, Random Forests    |\n",
        "| **White-Box** | Feature importance, PDP, ICE, SHAP (for extra visualization) | Add depth or visualization to interpretable models |\n",
        "\n",
        "---\n",
        "\n",
        "## üìå When to Use Post-Hoc XAI?\n",
        "\n",
        "* When the model is **already trained** and **changing it is not practical**.\n",
        "* When you need to **justify** model decisions to users, clients, or regulators.\n",
        "* When the model is complex and not understandable as-is (black-box).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "mV10f6jYg-76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Explainable AI (XAI) by its scope**\n",
        "\n",
        "## üéØ XAI by Scope: Overview\n",
        "\n",
        "There are **three main scopes** or levels in Explainable AI:\n",
        "\n",
        "| Scope                            | Description                                      | Example Question                                      |\n",
        "| -------------------------------- | ------------------------------------------------ | ----------------------------------------------------- |\n",
        "| **Global Explainability**        | Understand the **overall behavior** of the model | \"How does this model generally make decisions?\"       |\n",
        "| **Local Explainability**         | Understand a **single prediction** or decision   | \"Why did the model predict **this specific result**?\" |\n",
        "| **Example-Based Explainability** | Understand using **reference examples**          | \"What similar past cases influenced this prediction?\" |\n",
        "\n",
        "---\n",
        "\n",
        "## üîç 1. Global Explainability\n",
        "\n",
        "### üìå What it means:\n",
        "\n",
        "* Gives a **broad view** of how the model behaves overall.\n",
        "* Explains **which features matter most** and **how** they influence predictions.\n",
        "\n",
        "### ‚úÖ Tools:\n",
        "\n",
        "* **Feature Importance (e.g., SHAP summary plots)**\n",
        "* **Partial Dependence Plots (PDP)**\n",
        "* **Model summary visualization**\n",
        "\n",
        "### üß† Example:\n",
        "\n",
        "Predicting loan approval:\n",
        "\n",
        "* Global explanation shows:\n",
        "\n",
        "  * Income ‚Üí most important\n",
        "  * Age ‚Üí moderately important\n",
        "  * Education ‚Üí less important\n",
        "\n",
        "You understand **how the model behaves for all inputs.**\n",
        "\n",
        "---\n",
        "\n",
        "## üîç 2. Local Explainability\n",
        "\n",
        "### üìå What it means:\n",
        "\n",
        "* Focuses on **why the model made one specific prediction**.\n",
        "* Helpful for **trust and accountability** in individual decisions.\n",
        "\n",
        "### ‚úÖ Tools:\n",
        "\n",
        "* **LIME**\n",
        "* **SHAP (for a single instance)**\n",
        "* **Counterfactual explanations** (what would change the result)\n",
        "\n",
        "### üß† Example:\n",
        "\n",
        "A customer‚Äôs loan was denied.\n",
        "\n",
        "* Local explanation:\n",
        "\n",
        "  * Income = +15%\n",
        "  * Credit score = -40%\n",
        "  * Job type = -10%\n",
        "    ‚Üí Final prediction: Denied\n",
        "\n",
        "You now know **why this one person got denied**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç 3. Example-Based Explainability\n",
        "\n",
        "### üìå What it means:\n",
        "\n",
        "* Uses **similar examples** from the training set to explain a new decision.\n",
        "* Based on **‚Äúthis is similar to that‚Äù** logic.\n",
        "\n",
        "### ‚úÖ Tools:\n",
        "\n",
        "* **Case-based reasoning**\n",
        "* **K-Nearest Neighbors**\n",
        "* **Prototype/critic networks (like ProtoPNet)**\n",
        "\n",
        "### üß† Example:\n",
        "\n",
        "* Image classifier says: \"This is a tiger.\"\n",
        "* It shows 3 images from the training set that were also tigers and looked similar.\n",
        "* Human says: \"Okay, now I see why!\"\n",
        "\n",
        "---\n",
        "\n",
        "## üßæ Summary Table\n",
        "\n",
        "| Scope             | Goal                   | Tool Example              | Best For                               |\n",
        "| ----------------- | ---------------------- | ------------------------- | -------------------------------------- |\n",
        "| **Global**        | Understand whole model | SHAP summary, PDP         | Model debugging, feature understanding |\n",
        "| **Local**         | Explain one prediction | LIME, SHAP (single point) | High-stakes decisions                  |\n",
        "| **Example-Based** | Use similar examples   | ProtoPNet, KNN            | Interpretability through resemblance   |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Final Thought\n",
        "\n",
        "> XAI by scope helps you **zoom in or out**:\n",
        "\n",
        "* **Zoom out** = Global view of model behavior\n",
        "* **Zoom in** = Local view of one decision\n",
        "* **Compare** = Example-based explanations using known cases\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mu8KSEoaiSFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**SHAP** and **LIME**\n",
        "\n",
        "## üåü What Are SHAP and LIME?\n",
        "\n",
        "| Tool     | Full Form                                       | Purpose                                                        |\n",
        "| -------- | ----------------------------------------------- | -------------------------------------------------------------- |\n",
        "| **SHAP** | SHapley Additive exPlanations                   | Explains model output using feature contribution scores        |\n",
        "| **LIME** | Local Interpretable Model-agnostic Explanations | Builds a simple model locally to explain a specific prediction |\n",
        "\n",
        "Both are used to explain **black-box models** (e.g., neural nets, random forests, XGBoost).\n",
        "\n",
        "---\n",
        "\n",
        "## üîç 1. SHAP (SHapley Additive ExPlanations)\n",
        "\n",
        "### üí° Idea:\n",
        "\n",
        "SHAP uses **game theory** to fairly distribute credit to each feature in a prediction.\n",
        "\n",
        "Imagine each feature (age, income, gender, etc.) is like a player in a game. SHAP asks:\n",
        "\n",
        "> \"How much did each player (feature) contribute to the final score (prediction)?\"\n",
        "\n",
        "### üìå How It Works:\n",
        "\n",
        "* Calculates **all possible combinations** of features.\n",
        "* Measures how the prediction changes **with and without each feature**.\n",
        "* Averages the impact across all combinations ‚Üí **SHAP value**.\n",
        "\n",
        "### üìä Output:\n",
        "\n",
        "You get a **feature contribution score** ‚Äî positive values push the prediction up, negative values push it down.\n",
        "\n",
        "### üß† Example (Loan Default Prediction):\n",
        "\n",
        "Model prediction: 80% chance of loan default.\n",
        "\n",
        "| Feature            | SHAP Value | Contribution                               |\n",
        "| ------------------ | ---------- | ------------------------------------------ |\n",
        "| Credit score (low) | +20%       | Increased risk                             |\n",
        "| Income (low)       | +10%       | Increased risk                             |\n",
        "| Age (young)        | +5%        | Increased risk                             |\n",
        "| Total SHAP         | +35%       | Base value = 45%, so final = 45 + 35 = 80% |\n",
        "\n",
        "You now know **exactly why** the model predicted default.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç 2. LIME (Local Interpretable Model-Agnostic Explanations)\n",
        "\n",
        "### üí° Idea:\n",
        "\n",
        "LIME builds a simple, interpretable model (like a **linear model**) around a **specific prediction** to explain what mattered most **locally**.\n",
        "\n",
        "Think of it like:\n",
        "\n",
        "> \"Let's zoom in to just one point and approximate the model‚Äôs behavior near it.\"\n",
        "\n",
        "### üìå How It Works:\n",
        "\n",
        "1. Pick a data point you want to explain.\n",
        "2. Slightly change its input values (perturb).\n",
        "3. Get model predictions on these new points.\n",
        "4. Fit a simple model (e.g., linear regression) on these to **mimic the black-box** near that point.\n",
        "5. Use that model to explain feature importance.\n",
        "\n",
        "### üß† Example (Sentiment Analysis):\n",
        "\n",
        "Text: **‚ÄúThe movie was not good‚Äù**\n",
        "\n",
        "LIME perturbs it:\n",
        "\n",
        "* ‚ÄúThe movie was good‚Äù\n",
        "* ‚ÄúThe movie was not bad‚Äù\n",
        "* ‚ÄúThe movie was not good at all‚Äù\n",
        "\n",
        "Then it fits a linear model and finds:\n",
        "\n",
        "| Word   | Weight |\n",
        "| ------ | ------ |\n",
        "| ‚Äúnot‚Äù  | -0.8   |\n",
        "| ‚Äúgood‚Äù | +0.4   |\n",
        "\n",
        "Prediction: Negative sentiment because ‚Äúnot‚Äù has strong negative weight.\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Summary Table: SHAP vs LIME\n",
        "\n",
        "| Feature        | **SHAP**                         | **LIME**                 |\n",
        "| -------------- | -------------------------------- | ------------------------ |\n",
        "| **Based on**   | Game theory                      | Local surrogate modeling |\n",
        "| **Scope**      | Local + Global                   | Only Local               |\n",
        "| **Accuracy**   | Consistent, mathematically fair  | Approximate              |\n",
        "| **Output**     | Contribution values per feature  | Local feature weights    |\n",
        "| **Works with** | Any model (tree-based, NN, etc.) | Any model                |\n",
        "| **Complexity** | Slower, more accurate            | Faster, less precise     |\n",
        "\n",
        "---\n",
        "\n",
        "## üñºÔ∏è Visual Example (If You're Imagining a Plot)\n",
        "\n",
        "For SHAP:\n",
        "\n",
        "* A **bar plot** showing positive/negative contributions of each feature to a prediction.\n",
        "\n",
        "For LIME:\n",
        "\n",
        "* A **local linear line** around the prediction point, showing how nearby inputs affect the output.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† When to Use Which?\n",
        "\n",
        "| Use Case                               | Recommendation                                                   |\n",
        "| -------------------------------------- | ---------------------------------------------------------------- |\n",
        "| Need both global + local explanation   | **SHAP**                                                         |\n",
        "| Fast local explanation on one instance | **LIME**                                                         |\n",
        "| Tree-based models (e.g., XGBoost)      | **SHAP TreeExplainer** is perfect                                |\n",
        "| Text/image data                        | Both SHAP and LIME work, but **LIME** is often used for **text** |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kBNW6PuGjhkk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PDP (Partial Dependence Plot)**\n",
        "\n",
        "## üîç What is a Partial Dependence Plot (PDP)?\n",
        "\n",
        "A **Partial Dependence Plot (PDP)** shows how **a single feature** (or two) affects the **predicted output** of a machine learning model, **on average**.\n",
        "\n",
        "> It tells you:\n",
        "> \"**How does the model's prediction change as one feature changes, while keeping all others constant?**\"\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why Use PDP?\n",
        "\n",
        "PDP helps you:\n",
        "\n",
        "* Understand **global behavior** of the model\n",
        "* See whether a feature has a **positive or negative** effect\n",
        "* Detect **non-linear relationships** (e.g., U-shapes, thresholds)\n",
        "* Build trust in model predictions\n",
        "\n",
        "---\n",
        "\n",
        "## üìä PDP Example (Real-Life: House Price Prediction)\n",
        "\n",
        "### üìå Problem:\n",
        "\n",
        "You‚Äôve trained a model to **predict house prices**.\n",
        "Features include:\n",
        "\n",
        "* `Area (sq ft)`\n",
        "* `Location`\n",
        "* `No. of bedrooms`\n",
        "* `Age of the house`\n",
        "\n",
        "You want to know:\n",
        "\n",
        "> ‚ÄúHow does `Area` affect predicted price, regardless of other features?‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ How PDP Works:\n",
        "\n",
        "1. Pick a feature, e.g., `Area`.\n",
        "2. For a range of area values (say 500 to 3000 sq ft):\n",
        "\n",
        "   * Fix area to that value across **all** data points.\n",
        "   * Keep other features unchanged.\n",
        "   * Predict the price for each.\n",
        "   * Take the **average prediction**.\n",
        "3. Plot `Area` on the X-axis and **average predicted price** on Y-axis.\n",
        "\n",
        "---\n",
        "\n",
        "## üìà PDP Plot (What You‚Äôd See)\n",
        "\n",
        "| Area (sq ft) | Avg Predicted Price |\n",
        "| ------------ | ------------------- |\n",
        "| 500          | \\$80,000            |\n",
        "| 1000         | \\$120,000           |\n",
        "| 1500         | \\$180,000           |\n",
        "| 2000         | \\$250,000           |\n",
        "| 2500         | \\$310,000           |\n",
        "| 3000         | \\$350,000           |\n",
        "\n",
        "### üîº You'd get a graph like:\n",
        "\n",
        "```\n",
        "Price ‚Üë\n",
        "      |\n",
        "350K  |                               *\n",
        "310K  |                          *\n",
        "250K  |                   *\n",
        "180K  |            *\n",
        "120K  |      *\n",
        " 80K  | *\n",
        "      +-------------------------------- Area ‚Üí\n",
        "        500   1000  1500  2000  2500  3000\n",
        "```\n",
        "\n",
        "### Interpretation:\n",
        "\n",
        "> As `Area` increases, the predicted price increases ‚Äî the model has learned a **positive relationship** between area and house price.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Another Insightful Example (Binary Classification)\n",
        "\n",
        "### üìå Problem: Predicting **Loan Approval**\n",
        "\n",
        "* Model predicts `1 = approved`, `0 = rejected`\n",
        "\n",
        "You plot PDP for `credit score`.\n",
        "\n",
        "* X-axis: credit score from 400 to 850\n",
        "* Y-axis: average predicted probability of approval\n",
        "\n",
        "**If the curve is flat between 600‚Äì700 and jumps after 700**, it means:\n",
        "\n",
        "> ‚ÄúCredit score only improves approval chance significantly after it crosses 700.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Advantages of PDP\n",
        "\n",
        "| Benefit                         | Description                                                   |\n",
        "| ------------------------------- | ------------------------------------------------------------- |\n",
        "| **Global view**                 | Shows how a feature influences predictions across the dataset |\n",
        "| **Works with black-box models** | Supports Random Forest, XGBoost, etc.                         |\n",
        "| **Simple to interpret**         | Visual, continuous effect                                     |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö† Limitations\n",
        "\n",
        "| Limitation                 | Explanation                                                              |\n",
        "| -------------------------- | ------------------------------------------------------------------------ |\n",
        "| **Assumes independence**   | PDP may mislead if features are correlated (e.g., area and no. of rooms) |\n",
        "| **Averages over all data** | Can hide individual differences                                          |\n",
        "\n",
        "---\n",
        "\n",
        "## üõ† Tools to Create PDP\n",
        "\n",
        "* `sklearn.inspection.plot_partial_dependence` (Scikit-learn)\n",
        "* `pdpbox` library in Python\n",
        "* `SHAP` also includes partial dependence capability\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Summary\n",
        "\n",
        "| Term           | Meaning                                                                              |\n",
        "| -------------- | ------------------------------------------------------------------------------------ |\n",
        "| **PDP**        | Shows how one feature affects the model output **on average**, keeping others fixed  |\n",
        "| **Use it for** | Global understanding of model behavior                                               |\n",
        "| **Example**    | ‚ÄúHow does area affect house price?‚Äù or ‚ÄúHow does credit score affect loan approval?‚Äù |\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5axfpzYPqw9F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh4-ekxfQvLa"
      },
      "outputs": [],
      "source": []
    }
  ]
}