{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHB08JUZ1QknSTxBDugfYb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samiha-mahin/A-Machine-Learning-Models-Repo/blob/main/Probabilistic_Generative_%26_Probabilistic_Discriminative_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Probabilistic Generative Models** and **Probabilistic Discriminative Models**\n",
        "\n",
        "## üîµ 1. Probabilistic Generative Models\n",
        "\n",
        "### üìå What it means:\n",
        "\n",
        "These models **learn how each class generates the data**. They estimate the **joint probability**:\n",
        "\n",
        "$$\n",
        "P(x, y) = P(x | y) \\cdot P(y)\n",
        "$$\n",
        "\n",
        "Then use Bayes‚Äô Theorem to compute the **posterior**:\n",
        "\n",
        "$$\n",
        "P(y | x) = \\frac{P(x | y) \\cdot P(y)}{P(x)}\n",
        "$$\n",
        "\n",
        "### ‚úÖ Key Features:\n",
        "\n",
        "* Model the **distribution of inputs** for each class.\n",
        "* Can **generate new data** samples (hence ‚Äúgenerative‚Äù).\n",
        "* Works well when class-conditional distributions are well-separated.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Common Generative Models:\n",
        "\n",
        "| Model                                     | Description                                                                      | Usage Example                                         |\n",
        "| ----------------------------------------- | -------------------------------------------------------------------------------- | ----------------------------------------------------- |\n",
        "| **Gaussian Naive Bayes**                  | Assumes features are independent within each class and normally distributed      | Text classification, spam detection                   |\n",
        "| **LDA (Linear Discriminant Analysis)**    | Assumes each class has a Gaussian distribution with **shared covariance** matrix | Image recognition, early-stage medical classification |\n",
        "| **QDA (Quadratic Discriminant Analysis)** | Same as LDA but allows **each class to have its own covariance** matrix          | Better for non-linear boundaries                      |\n",
        "| **Gaussian Mixture Models (GMM)**         | Unsupervised generative model (uses EM algorithm) to model data                  | Clustering with soft labels, speaker recognition      |\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Equation Example:\n",
        "\n",
        "Given:\n",
        "\n",
        "$$\n",
        "p(x|C_k) \\sim \\mathcal{N}(x | \\mu_k, \\Sigma)\n",
        "$$\n",
        "\n",
        "Then, posterior:\n",
        "\n",
        "$$\n",
        "P(C_1 | x) = \\sigma(w^T x + w_0)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $w = \\Sigma^{-1} (\\mu_1 - \\mu_2)$\n",
        "* $w_0 = -\\frac{1}{2} \\mu_1^T \\Sigma^{-1} \\mu_1 + \\frac{1}{2} \\mu_2^T \\Sigma^{-1} \\mu_2 + \\ln \\frac{P(C_1)}{P(C_2)}$\n",
        "\n",
        "This is how **generative model ends up using logistic-like posterior**, but it's still a generative approach.\n",
        "\n",
        "---\n",
        "\n",
        "## üî¥ 2. Probabilistic Discriminative Models\n",
        "\n",
        "### üìå What it means:\n",
        "\n",
        "These models **directly model the posterior** probability:\n",
        "\n",
        "$$\n",
        "P(y | x)\n",
        "$$\n",
        "\n",
        "without learning how the data was generated.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Key Features:\n",
        "\n",
        "* Focus on the **decision boundary** directly.\n",
        "* Often more **accurate** than generative models when the goal is just classification.\n",
        "* Can't generate new data.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Common Discriminative Models:\n",
        "\n",
        "| Model                      | Description                                                           | Usage Example                       |                                          |\n",
        "| -------------------------- | --------------------------------------------------------------------- | ----------------------------------- | ---------------------------------------- |\n",
        "| **Logistic Regression**    | Models (P(y                                                           | x)) using sigmoid/logistic function | Binary classification, medical diagnosis |\n",
        "| **Softmax Regression**     | Extension of logistic regression for multi-class problems             | Handwritten digit recognition       |                                          |\n",
        "| **Neural Networks**        | Powerful discriminative models that learn complex decision boundaries | Deep learning, image classification |                                          |\n",
        "| **Support Vector Machine** | Maximizes margin between classes, often used with kernels             | Text classification, bioinformatics |                                          |\n",
        "\n",
        "---\n",
        "\n",
        "## üü† Quick Summary Table:\n",
        "\n",
        "| Feature           | Generative Model                        | Discriminative Model         |       |\n",
        "| ----------------- | --------------------------------------- | ---------------------------- | ----- |\n",
        "| Learns P(x        | y) + P(y)                               | ‚úÖ Yes                        | ‚ùå No  |\n",
        "| Learns P(y        | x) directly                             | ‚ùå No (uses Bayes)            | ‚úÖ Yes |\n",
        "| Can generate data | ‚úÖ Yes                                   | ‚ùå No                         |       |\n",
        "| Model example     | LDA, QDA, Naive Bayes, GMM              | Logistic Regression, SVM, NN |       |\n",
        "| Better for        | Small data, inference, generative tasks | Classification accuracy      |       |\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Which One Should You Use?\n",
        "\n",
        "| Situation                         | Model to Prefer       |\n",
        "| --------------------------------- | --------------------- |\n",
        "| Simple and interpretable task     | Logistic Regression   |\n",
        "| Well-separated Gaussian data      | LDA or QDA            |\n",
        "| High-dimensional sparse text data | Naive Bayes           |\n",
        "| Complex nonlinear classification  | Neural Networks / SVM |\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SoaeYVgljGv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**QDA (Quadratic Discriminant Analysis)**\n",
        "\n",
        "## üîµ What is QDA?\n",
        "\n",
        "### ‚úÖ QDA = **Quadratic Discriminant Analysis**\n",
        "\n",
        "QDA is a **probabilistic generative model** used for **classification**. It assumes:\n",
        "\n",
        "* Each class generates data following a **multivariate Gaussian distribution**.\n",
        "* Each class has its **own covariance matrix**.\n",
        "\n",
        "This makes QDA more flexible than LDA (Linear Discriminant Analysis), which assumes **shared covariance**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ The Math Behind QDA:\n",
        "\n",
        "For class $C_k$, we assume:\n",
        "\n",
        "$$\n",
        "p(x | C_k) \\sim \\mathcal{N}(x | \\mu_k, \\Sigma_k)\n",
        "$$\n",
        "\n",
        "Then, the posterior (using Bayes' theorem) is:\n",
        "\n",
        "$$\n",
        "P(C_k | x) = \\frac{P(C_k) \\cdot \\mathcal{N}(x | \\mu_k, \\Sigma_k)}{\\sum_j P(C_j) \\cdot \\mathcal{N}(x | \\mu_j, \\Sigma_j)}\n",
        "$$\n",
        "\n",
        "The decision boundary is found by comparing:\n",
        "\n",
        "$$\n",
        "\\log P(C_1 | x) = \\log P(C_2 | x)\n",
        "$$\n",
        "\n",
        "Due to different covariances ($\\Sigma_k$), the boundary turns out to be a **quadratic surface**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Key Differences: LDA vs QDA\n",
        "\n",
        "| Feature             | LDA                                  | QDA                           |\n",
        "| ------------------- | ------------------------------------ | ----------------------------- |\n",
        "| Covariance matrix   | Shared across all classes ($\\Sigma$) | Unique per class ($\\Sigma_k$) |\n",
        "| Decision boundary   | Linear                               | Quadratic                     |\n",
        "| Flexibility         | Less flexible                        | More flexible                 |\n",
        "| Risk of overfitting | Lower                                | Higher (if less data)         |\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## üìà Visualization (How the Boundary Looks)\n",
        "\n",
        "QDA produces **curved/quadratic boundaries** between classes. This helps when:\n",
        "\n",
        "* The class clouds are **not linearly separable**\n",
        "* One class is more spread out or rotated than the other\n",
        "\n",
        "![Visual Concept ‚Äî QDA](https://scikit-learn.org/stable/_images/sphx_glr_plot_lda_qda_001.png)\n",
        "\n",
        "*(Blue vs red: LDA = straight line, QDA = curved line)*\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ When to Use QDA?\n",
        "\n",
        "| Use QDA When...                                        |\n",
        "| ------------------------------------------------------ |\n",
        "| Classes are **not linearly separable**                 |\n",
        "| You suspect **different covariance shapes per class**  |\n",
        "| You have **enough data** to estimate multiple matrices |\n",
        "\n",
        "### ‚ö†Ô∏è Avoid QDA if:\n",
        "\n",
        "* You have **few samples** (too many parameters in QDA).\n",
        "* Your data is actually **well-separated linearly** ‚Äî LDA is safer.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary:\n",
        "\n",
        "| Aspect            | QDA Explanation                                                           |\n",
        "| ----------------- | ------------------------------------------------------------------------- |\n",
        "| Type              | Probabilistic Generative Classifier                                       |\n",
        "| Covariance        | Different per class                                                       |\n",
        "| Shape of Boundary | Quadratic                                                                 |\n",
        "| Pros              | More flexible than LDA                                                    |\n",
        "| Cons              | Prone to overfitting with small data                                      |\n",
        "| Real-world Use    | Complex medical diagnosis, speech recognition, handwriting classification |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "WziURQb4kDuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Use only 2 classes: Setosa (0) and Versicolor (1)\n",
        "mask = y < 2\n",
        "X = X[mask][:, :2]  # only first two features\n",
        "y = y[mask]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "# Apply QDA\n",
        "qda = QuadraticDiscriminantAnalysis()\n",
        "qda.fit(X_train_std, y_train)\n",
        "y_pred = qda.predict(X_test_std)\n",
        "\n",
        "# Accuracy\n",
        "print(\"QDA Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNVeS7bukZGa",
        "outputId": "768074e9-e604-4e8d-f3f6-280b3fcf176c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QDA Accuracy: 1.0\n"
          ]
        }
      ]
    }
  ]
}